import os
import json
import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import pickle
from datetime import datetime
import inspect
import re
from scipy.ndimage import label
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
# ignore runtime warning
warnings.filterwarnings("ignore", category=RuntimeWarning)

# root_folder_path = '/G/results/visualization_simulation_singclus/' #'/mnt/mimo_1/simu_results_sjc/simulation_singclus_Aug25'  #'/G/results/simulation/'
root_folder_path = '/mnt/mimo_1/simu_results_sjc/simulation_singclus_Aug25'  #'/G/results/simulation/'

# æ·»åŠ æ•°æ®ç¼“å­˜ä»¥åŠ é€Ÿé‡å¤åŠ è½½
_data_cache = {}

def load_data(exp):
    # æ£€æŸ¥ç¼“å­˜
    if exp in _data_cache:
        return _data_cache[exp]
    
    folder = os.path.join(root_folder_path, exp)
    dt = 1 / 40000

    # æ‰€æœ‰å¯èƒ½åŠ è½½çš„å˜é‡åä¸Žå¯¹åº”æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    npy_files = {
        'v': 'dend_v_array',
        'i': 'dend_i_array',
        'nmda': 'dend_nmda_i_array',
        'ampa': 'dend_ampa_i_array',
        'nmda_g': 'dend_nmda_g_array',
        'ampa_g': 'dend_ampa_g_array',
        'soma': 'soma_v_array',
        'apic_v': 'apic_v_array',
        'apic_ica': 'apic_ica_array',
        'soma_i': 'soma_i_array',
        'trunk_v': 'trunk_v_array',
        'basal_v': 'basal_v_array',
        'tuft_v': 'tuft_v_array',
        'basal_bg_i_nmda': 'basal_bg_i_nmda_array',
        'basal_bg_i_ampa': 'basal_bg_i_ampa_array',
        'tuft_bg_i_nmda': 'tuft_bg_i_nmda_array',
        'tuft_bg_i_ampa': 'tuft_bg_i_ampa_array',
    }

    data = {}

    for var_name, file_base in npy_files.items():
        file_path = os.path.join(folder, f"{file_base}.npy")
        if os.path.exists(file_path):
            data[var_name] = np.load(file_path)
        else:
            data[var_name] = None  # æˆ–è€…ä¸åŠ å…¥ï¼Œå¦‚æžœä½ æ›´å–œæ¬¢ dict.get(...)

    # åŠ è½½ simulation info
    with open(os.path.join(folder, 'simulation_params.json')) as f:
        simu_info = json.load(f)

    # åŠ è½½ section_synapse_df.csv
    sec_syn_df_path = os.path.join(folder, 'section_synapse_df.csv')
    if os.path.exists(sec_syn_df_path):
        sec_syn_df = pd.read_csv(sec_syn_df_path)
    else:
        sec_syn_df = None

    data['dt'] = dt
    data['simu_info'] = simu_info
    data['sec_syn_df'] = sec_syn_df

    # ç¼“å­˜æ•°æ®
    _data_cache[exp] = data
    return data

def nonlinearity_visualization(exp, data, rec_loc, attr, analyze_flag, plot_flag, plot_attr=None, alpha=1):

    if data is None:
        data = load_data(exp)
    v, soma, apic_v, dt = [data.get(k) for k in ('v', 'soma', 'apic_v', 'dt')]
    simu_info = data['simu_info']  # å¿…é¡»å­˜åœ¨
    
    if v.ndim == 5:
        v = np.mean(v, axis=2) # shape: [num_clusters, num_times, num_affs, num_trials]
        soma = np.mean(soma, axis=1) # shape: [num_times, num_affs, num_trials]
        apic_v = np.mean(apic_v, axis=1) # shape: [num_times, num_affs, num_trials]
        
    t = simu_info['time point of stimulation']
    t_start, t_end = (t-20)*40, (t+100)*40
    x = np.arange(0, t_end-t_start)*dt # for area calculation
    
    # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„åˆ‡ç‰‡å’Œreshapeæ“ä½œ
    v_base_trace = v[:,:,0:1,:]  # ä¿æŒç»´åº¦ï¼Œé¿å…reshape
    soma_base_trace = soma[:,0:1,:]
    apic_v_base_trace = apic_v[:,0:1,:]
    
    # Initialize variables
    EPSP_array = None
    nmda_flag_array = None
    
    if analyze_flag:
        # ä¼˜åŒ–ï¼šé¢„è®¡ç®—deltaå€¼ï¼Œé¿å…é‡å¤è®¡ç®—
        if rec_loc == 'dend':
            dend_delta = np.mean(v[:, t_start:t_end, :, :] - v_base_trace[:, t_start:t_end, :, :], axis=-1)  # shape: [num_clusters, t, num_affs]
            if attr == 'peak':
                EPSP_array = np.mean(np.max(dend_delta, axis=1), axis=0)  # [num_affs]
            elif attr == 'area':
                dend_over_baseline = np.clip(dend_delta, 1, None)  # [num_clus, t, num_affs]
                EPSP_array = np.mean(np.trapz(dend_over_baseline, x, axis=1), axis=0)

        elif rec_loc == 'soma':
            soma_delta = np.mean(soma[t_start:t_end, :, :] - soma_base_trace[t_start:t_end, :, :], axis=-1)  # shape: [t, num_affs]
            if attr == 'peak':
                EPSP_array = np.max(soma_delta, axis=0)  # [num_affs]

                if 'expected' in exp:
                    # ä¼˜åŒ–ï¼šå‘é‡åŒ–expectedè®¡ç®—
                    max_values = np.max(soma_delta, axis=0)
                    EPSP_array = [np.sum(max_values[:1+2*i]) for i in range(37)]

            elif attr == 'area':
                soma_over_baseline = np.clip(soma_delta, 0, None)  # [t, num_affs]
                EPSP_array = np.trapz(soma_over_baseline, x, axis=0)

        elif rec_loc == 'nexus':
            apic_v_delta = np.mean(apic_v[t_start:t_end, :, :] - apic_v_base_trace[t_start:t_end, :, :], axis=-1)  # shape: [t, num_affs]
            if attr == 'peak':
                EPSP_array = np.max(apic_v_delta , axis=0)  # [num_affs]

                if 'expected' in exp:
                    # ä¼˜åŒ–ï¼šå‘é‡åŒ–expectedè®¡ç®—
                    max_values = np.max(apic_v_delta, axis=0)
                    EPSP_array = [np.sum(max_values[:1+2*i]) for i in range(37)]

            elif attr == 'area':
                apic_v_over_baseline = np.clip(apic_v_delta , 0, None)  # [t, num_affs]
                EPSP_array = np.trapz(apic_v_over_baseline, x, axis=0)

        v_slice = v[0, t_start:t_end, :, 0]  # shape: (time, third_dim)
        nmda_flag_array = np.zeros(v_slice.shape[1], dtype=bool)

        nmda_v_thres = -40 # mV
        nmda_dur_thres = 26 * 40 # 1/40 ms
        for i in range(v_slice.shape[1]):
            labeled, num_features = label(v_slice[:, i] > nmda_v_thres)
            durations = np.bincount(labeled.ravel())[1:]  # æ›´å¿«åœ°ç»Ÿè®¡æ¯ä¸ªlabelé•¿åº¦
            nmda_flag_array[i] = int(np.any(durations >= nmda_dur_thres))

    if plot_flag and plot_attr is not None:
        # Extract plot attributes from dictionary
        ax_idx = plot_attr.get('ax_idx')
        exp_idx = plot_attr.get('exp_idx')
        fig = plot_attr.get('fig')
        ax = plot_attr.get('ax')
        
        syn_num_step = 1
        fig.subplots_adjust(wspace=0)
        ax[ax_idx//syn_num_step].set_title(f'{exp_idx+1}') # Label the subplot with the epoch index

        # é¢„è®¡ç®—syn_num_listï¼Œé¿å…é‡å¤è®¡ç®—
        if 'multiclus' in exp:
            syn_num_list = [0, 1, 3, 6, 12, 24, 48, 72]
        else:
            syn_num_list = list(range(0, 73, 2))

        color_dict = {'dend': 'C0', 'soma': 'k', 'nexus': 'b'}
        ax[ax_idx // syn_num_step].plot(syn_num_list, EPSP_array, color=color_dict.get(rec_loc, 'k'), alpha=alpha)
    
    # Only compute difference traces if needed for return (when plot_flag is True)
    if plot_flag:
        v_diff = v - v_base_trace
        soma_diff = soma - soma_base_trace
        apic_v_diff = apic_v - apic_v_base_trace
    else:
        # Return empty arrays to maintain return signature
        v_diff = soma_diff = apic_v_diff = None
        
    return v_diff, soma_diff, apic_v_diff, EPSP_array, nmda_flag_array

def full_nonlinearity_visualization(exp_list, idx_list, rec_loc_list, attr_list, num_epochs=10, analyze_flag=True, plot_flag=False, clus_nmda_flag_matrix=None):
    
    ### Load data ###
    exp, idx, rec_loc, attr = exp_list[0], idx_list[0], rec_loc_list[0], attr_list[0]
    
    if plot_flag:
        num_ax_rows = np.ceil(num_epochs/5).astype(int)
        num_subplot_per_row = np.ceil(num_epochs/num_ax_rows).astype(int)

        fig, ax = plt.subplots(num_ax_rows, 1+num_subplot_per_row, figsize=(3*(1+num_subplot_per_row), 4*num_ax_rows), sharey=False)
        ax = list(ax.flat) 
        plt.suptitle(exp + ' ' + str(idx) + ' ' + rec_loc, fontsize=18)
    
    data_dict = {}
    for epoch_idx in range(num_epochs):
        epoch_path = exp + '/' + str(idx) + '/' + str(epoch_idx + 1) + '/'
        data_dict[epoch_idx] = load_data(epoch_path)

    # Only create lists if plot_flag is True (for plotting averaged curves)
    if plot_flag:
        v_list, soma_list, apic_v_list = [], [], []
    
    EPSP_array_list, nmda_flag_array_list = [], []
    
    for epoch_idx in range(num_epochs):
        # Only create plot_attr if plot_flag is True
        plot_attr = None
        if plot_flag:
            plot_attr = {
                'ax_idx': epoch_idx,
                'exp_idx': epoch_idx,
                'fig': fig,
                'ax': ax
            }
        
        v, soma, apic_v, EPSP_array, nmda_flag_array = nonlinearity_visualization(exp + '/' + str(idx) + '/' + str(epoch_idx + 1) + '/', 
                                                                data_dict[epoch_idx], rec_loc, attr, analyze_flag, plot_flag, plot_attr)
        
        # Only compute these if plot_flag is True
        if plot_flag:
            v_list.append(np.mean(v, axis=(0, -1)))  # average over clusters and trials
            soma_list.append(np.mean(soma, axis=-1))  # average over trials
            apic_v_list.append(np.mean(apic_v, axis=-1))  # average over trials
        
        EPSP_array_list.append(EPSP_array)
        nmda_flag_array_list.append(nmda_flag_array)

    # ä¼˜åŒ–ï¼šä½¿ç”¨numpyæ•°ç»„æ“ä½œï¼Œé¿å…é‡å¤è½¬æ¢
    EPSP_matrix = np.array(EPSP_array_list) # shape: [num_epochs, num_affs]
    nmda_flag_matrix = np.array(nmda_flag_array_list) # shape: [num_epochs, num_affs]

    avg_EPSP_array = np.mean(EPSP_matrix, axis=0)
    std_EPSP_array = np.std(EPSP_matrix, axis=0)
    
    ### Plot the averaged curve ###
    if plot_flag:
        if 'multiclus' in exp:
            syn_num_list = [0, 1, 3, 6, 12, 24, 48, 72]
        else:
            syn_num_list = list(range(0, 73, 2))

        color_dict = {'dend': 'C0', 'soma': 'k', 'nexus': 'b'}

        ax[-1].plot(syn_num_list, avg_EPSP_array, color=color_dict.get(rec_loc, 'k'), alpha=1)
        ax[-1].fill_between(syn_num_list, avg_EPSP_array - std_EPSP_array, avg_EPSP_array + std_EPSP_array, color=color_dict.get(rec_loc, 'k'), alpha=0.2)
        ax[-1].set_title('avg')
        
        ### Unify the format across subplots ###
        # è®¾ç½® y è½´ä¸Šé™
        if rec_loc == 'dend':
            max_ylim_peak, max_ylim_area = 80, 8
        elif rec_loc in ['soma', 'nexus']:
            max_ylim_peak, max_ylim_area = 8, 0.4

        # ä¼˜åŒ–ï¼šé¢„è®¡ç®—å…¬å…±å‚æ•°
        xticks = list(range(0, 73, 12))
        num_axes = num_ax_rows * (1 + num_subplot_per_row)
        
        for ax_i in ax[:num_axes]:
            ax_i.set_xlabel('Number of Synapses')
            ax_i.set_xticks(xticks)

            # yè½´æ ‡ç­¾å’Œé™åˆ¶
            if attr == 'peak':
                ax_i.set_ylabel('EPSP (mV)')
                ylim = (-math.ceil(max_ylim_peak / 16), math.ceil(max_ylim_peak * 9 / 8))
                yticks = list(range(0, int(max_ylim_peak * 9 / 8), int(max_ylim_peak / 4)))
            elif attr == 'area':
                ax_i.set_ylabel('EPSP Area (mV ms)')
                ylim = (-max_ylim_area / 16, max_ylim_area * 9 / 8)
                yticks = np.arange(0, max_ylim_area * 9 / 8, max_ylim_area / 4)

            ax_i.set_ylim(*ylim)
            ax_i.set_yticks(yticks)

            # ç¾ŽåŒ–
            ax_i.spines['top'].set_visible(False)
            ax_i.spines['right'].set_visible(False)

        fig.tight_layout()

        # Save the figure
        # os.makedirs('/G/results/simulation/full_nonlinearity_visualization', exist_ok=True)
        # plt.savefig(f'/G/results/simulation/full_nonlinearity_visualization/{exp_list[0]}_{rec_loc}_{attr}.pdf', dpi=300, bbox_inches='tight')
        plt.close(fig)

    return avg_EPSP_array, EPSP_matrix, nmda_flag_matrix

class GlobalVarManager:
    def __init__(self, name_pattern=None, exclude_vars=None):
        """
        åˆå§‹åŒ–å…¨å±€å˜é‡ç®¡ç†å™¨
        
        Args:
            name_pattern: å˜é‡ååŒ¹é…çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
            exclude_vars: è¦æŽ’é™¤çš„å˜é‡ååˆ—è¡¨
        """
        self.name_pattern = name_pattern
        self.exclude_vars = exclude_vars or [
            '__builtins__', '__cached__', '__doc__', '__file__', 
            '__loader__', '__name__', '__package__', '__spec__',
            'inspect', 'pickle', 'os', 'datetime', 'GlobalVarManager'
        ]
    
    def get_all_globals(self):
        """èŽ·å–å½“å‰æ‰€æœ‰å…¨å±€å˜é‡"""
        globals_dict = {}
        current_globals = globals()
        
        for var_name, var_value in current_globals.items():
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å˜é‡å
            if self.name_pattern and not re.search(self.name_pattern, var_name, re.IGNORECASE):
                continue
            
            # æŽ’é™¤å†…ç½®å˜é‡å’ŒæŒ‡å®šçš„æŽ’é™¤å˜é‡
            if (not var_name.startswith('__') and 
                var_name not in self.exclude_vars and
                not inspect.ismodule(var_value) and
                not inspect.isfunction(var_value) and
                not inspect.isclass(var_value)):
                
                try:
                    pickle.dumps(var_value)
                    globals_dict[var_name] = var_value
                except (pickle.PicklingError, TypeError, AttributeError):
                    print(f"è­¦å‘Š: å˜é‡ '{var_name}' æ— æ³•åºåˆ—åŒ–ï¼Œå·²è·³è¿‡")
                    continue
        
        return globals_dict
    
    def save_globals(self, filename=None, include_timestamp=True):
        """
        ä¿å­˜æ‰€æœ‰å…¨å±€å˜é‡åˆ°pickleæ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶åï¼Œå¦‚æžœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            include_timestamp: æ˜¯å¦åœ¨æ–‡ä»¶åä¸­åŒ…å«æ—¶é—´æˆ³
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"globals_backup_{timestamp}.pkl"
        
        globals_dict = self.get_all_globals()
        
        # æ·»åŠ å…ƒæ•°æ®
        metadata = {
            'saved_at': datetime.now().isoformat(),
            'variables_count': len(globals_dict),
            'variable_names': list(globals_dict.keys())
        }
        
        save_data = {
            'metadata': metadata,
            'globals': globals_dict
        }
        
        try:
            with open(filename, 'wb') as f:
                pickle.dump(save_data, f)
            print(f"âœ… æˆåŠŸä¿å­˜ {len(globals_dict)} ä¸ªå…¨å±€å˜é‡åˆ°: {filename}")
            print(f"ðŸ“‹ ä¿å­˜çš„å˜é‡: {list(globals_dict.keys())}")
            return filename
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return None
    
    def load_globals(self, filename, overwrite_existing=True):
        """
        ä»Žpickleæ–‡ä»¶åŠ è½½å…¨å±€å˜é‡
        
        Args:
            filename: è¦åŠ è½½çš„æ–‡ä»¶å
            overwrite_existing: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„å˜é‡
        """
        try:
            with open(filename, 'rb') as f:
                save_data = pickle.load(f)
            
            metadata = save_data.get('metadata', {})
            globals_dict = save_data.get('globals', {})
            
            print(f"ðŸ“‚ ä»Žæ–‡ä»¶åŠ è½½: {filename}")
            print(f"â° ä¿å­˜æ—¶é—´: {metadata.get('saved_at', 'æœªçŸ¥')}")
            print(f"ðŸ“Š å˜é‡æ•°é‡: {metadata.get('variables_count', len(globals_dict))}")
            
            loaded_count = 0
            skipped_count = 0
            
            for var_name, var_value in globals_dict.items():
                if var_name in globals() and not overwrite_existing:
                    print(f"âš ï¸  è·³è¿‡å·²å­˜åœ¨çš„å˜é‡: {var_name}")
                    skipped_count += 1
                    continue
                
                globals()[var_name] = var_value
                loaded_count += 1
            
            print(f"âœ… æˆåŠŸåŠ è½½ {loaded_count} ä¸ªå˜é‡")
            if skipped_count > 0:
                print(f"âš ï¸  è·³è¿‡ {skipped_count} ä¸ªå·²å­˜åœ¨çš„å˜é‡")
            
            return True
            
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            return False
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return False
    
    def list_saved_files(self, directory='.'):
        """åˆ—å‡ºæ‰€æœ‰ä¿å­˜çš„å…¨å±€å˜é‡æ–‡ä»¶"""
        files = [f for f in os.listdir(directory) if f.startswith('globals_backup_') and f.endswith('.pkl')]
        files.sort(reverse=True)  # æœ€æ–°çš„æ–‡ä»¶åœ¨å‰
        
        if not files:
            print("ðŸ“ æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„å…¨å±€å˜é‡æ–‡ä»¶")
            return []
        
        print("ðŸ“ æ‰¾åˆ°ä»¥ä¸‹ä¿å­˜çš„å…¨å±€å˜é‡æ–‡ä»¶:")
        for i, file in enumerate(files, 1):
            file_path = os.path.join(directory, file)
            file_size = os.path.getsize(file_path)
            file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
            print(f"  {i}. {file} ({file_size/1024:.1f} KB, {file_time.strftime('%Y-%m-%d %H:%M:%S')})")
        
        return files

def save_epsps_global():
    """åªä¿å­˜EPSPç›¸å…³çš„å…¨å±€å˜é‡"""
    gvm = GlobalVarManager(name_pattern=r'.*EPSP.*')  # åŒ¹é…åŒ…å«EPSPçš„å˜é‡
    
    # èŽ·å–æ‰€æœ‰EPSPç›¸å…³å˜é‡
    epsp_vars = gvm.get_all_globals()
    
    if not epsp_vars:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å«'EPSP'çš„å˜é‡")
        return None
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"epsps_backup_{timestamp}.pkl"
    
    return gvm.save_globals(filename)

def load_epsps_global(filename=None):
    """åŠ è½½EPSPç›¸å…³çš„å…¨å±€å˜é‡"""
    gvm = GlobalVarManager(name_pattern=r'.*EPSP.*')  # åŒ¹é…åŒ…å«EPSPçš„å˜é‡
    
    if filename is None:
        files = [f for f in os.listdir('.') if f.startswith('epsps_backup_') and f.endswith('.pkl')]
        if files:
            filename = sorted(files, reverse=True)[0]
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°EPSPå˜é‡ä¿å­˜æ–‡ä»¶")
            return False
    
    return gvm.load_globals(filename)

def _single_analysis_task(task_params):
    """
    Wrapper function for a single analysis task (used for parallel processing).
    """
    prefix, filename_template, anal_loc, rec_loc, attr, range_idx, \
        num_epochs, iter_start_idx, iter_end_idx, iter_step = task_params
    
    iter_times = iter_end_idx - iter_start_idx
    
    var_base = f'{prefix}_{anal_loc}_{attr}_{rec_loc}_{range_idx}'
    
    try:
        epsp_array, epsp_matrix, nmda_flag_matrix = full_nonlinearity_visualization(
            [anal_loc + f'_range{range_idx}' + filename_template] * iter_times,
            list(range(iter_start_idx, iter_end_idx, iter_step)),
            [rec_loc] * iter_times,
            [attr] * iter_times,
            num_epochs=num_epochs
        )
        
        return var_base, epsp_array, epsp_matrix, nmda_flag_matrix
    except Exception as e:
        print(f'âŒ Error processing {var_base}: {e}')
        return var_base, None, None, None

def batch_nonlinearity_analysis(prefix, filename_template, anal_loc_list, rec_loc_list_map, 
                                  num_epochs=50, iter_start_idx=1, iter_end_idx=2, iter_step=1,
                                  parallel=True, max_workers=5):
    """
    Generic function to batch run nonlinearity analysis for different conditions.
    
    Parameters:
    -----------
    prefix : str
        Variable name prefix (e.g., 'vitro_N+A', 'vitro_A_distr')
    filename_template : str  
        Filename template (e.g., '_clus_invitro_singclus', '_distr_invitro_singclus_AMPA')
    anal_loc_list : list
        List of anatomical locations (e.g., ['basal', 'apical'])
    rec_loc_list_map : dict
        Mapping from anal_loc to list of rec_locs (e.g., {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']})
    num_epochs : int
        Number of epochs to analyze
    iter_start_idx : int
        Starting index for iterations
    iter_end_idx : int
        Ending index for iterations
    iter_step : int
        Step size for iterations
    parallel : bool
        Whether to use parallel processing (default: True)
    max_workers : int
        Maximum number of worker processes (default: CPU count)
    """
    iter_times = iter_end_idx - iter_start_idx
    
    # Generate all task parameters
    task_list = []
    for anal_loc in anal_loc_list:
        rec_loc_list = rec_loc_list_map.get(anal_loc, ['dend', 'soma'])
        
        for attr in ['peak', 'area']:
            for rec_loc in rec_loc_list:
                for range_idx in range(3):
                    task_params = (
                        prefix, filename_template, anal_loc, rec_loc, attr, range_idx,
                        num_epochs, iter_start_idx, iter_end_idx, iter_step
                    )
                    task_list.append(task_params)
    
    print(f'ðŸ“Š Starting batch analysis: {len(task_list)} tasks')
    print(f'âš™ï¸  Parallel mode: {parallel}')
    
    if parallel and len(task_list) > 1:
        # Parallel processing
        if max_workers is None:
            max_workers = min(mp.cpu_count(), len(task_list))
        
        print(f'ðŸš€ Using {max_workers} workers')
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {executor.submit(_single_analysis_task, task): task for task in task_list}
            
            completed = 0
            for future in as_completed(future_to_task):
                completed += 1
                try:
                    var_base, epsp_array, epsp_matrix, nmda_flag_matrix = future.result()
                    
                    if epsp_array is not None:
                        globals()[f'{var_base}_EPSP_array'] = epsp_array
                        globals()[f'{var_base}_EPSP_matrix'] = epsp_matrix
                        globals()[f'{var_base}_nmda_flag_matrix'] = nmda_flag_matrix
                        print(f'âœ“ [{completed}/{len(task_list)}] Completed: {var_base}')
                    else:
                        print(f'âœ— [{completed}/{len(task_list)}] Failed: {var_base}')
                except Exception as e:
                    print(f'âœ— [{completed}/{len(task_list)}] Error: {e}')
    else:
        # Sequential processing
        print(f'ðŸ”„ Sequential mode')
        for task in task_list:
            var_base, epsp_array, epsp_matrix, nmda_flag_matrix = _single_analysis_task(task)
            
            if epsp_array is not None:
                globals()[f'{var_base}_EPSP_array'] = epsp_array
                globals()[f'{var_base}_EPSP_matrix'] = epsp_matrix
                globals()[f'{var_base}_nmda_flag_matrix'] = nmda_flag_matrix
                print(f'âœ“ Completed: {var_base}')
    
    print(f'âœ… Batch analysis completed!')

#### All conditions in one call:

# Complete example for all vitro conditions
iter_start_idx, iter_end_idx = 1, 2
iter_step, num_epochs = 1, 50

# 1. Vitro N+A Clustered & Distributed
batch_nonlinearity_analysis('vitro_N+A', '_clus_invitro_singclus', ['basal', 'apical'], 
                            {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)
 
# batch_nonlinearity_analysis('vitro_N+A_distr', '_distr_invitro_singclus', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)

# # 2. Vitro AMPA Clustered & Distributed
# batch_nonlinearity_analysis('vitro_A', '_clus_invitro_singclus_AMPA', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)

# batch_nonlinearity_analysis('vitro_A_distr', '_distr_invitro_singclus_AMPA', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)

# # 3. Vitro N/A 3:1 Clustered & Distributed
# batch_nonlinearity_analysis('vitro_N/A_3', '_clus_invitro_singclus_ratio3', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)

# batch_nonlinearity_analysis('vitro_N/A_3_distr', '_distr_invitro_singclus_ratio3', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)

# # 4. Vivo N+A Clustered & Distributed
# batch_nonlinearity_analysis('vivo_N+A', '_clus_invivo_singclus', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)

# batch_nonlinearity_analysis('vivo_N+A_distr', '_distr_invivo_singclus', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)

# # 5. Vivo AMPA Clustered & Distributed
# batch_nonlinearity_analysis('vivo_A', '_clus_invivo_singclus_AMPA', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)

# batch_nonlinearity_analysis('vivo_A_distr', '_distr_invivo_singclus_AMPA', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs)

# root_folder_path = '/G/results/simulation_singclus_Oct25'

# # 6. Vivo N+A 10ms Clustered & Distributed
# batch_nonlinearity_analysis('vivo_N+A_10ms', '_clus_invivo_singclus_t10ms', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs=10)

# batch_nonlinearity_analysis('vivo_N+A_10ms_distr', '_distr_invivo_singclus_t10ms', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs=10)

# # 7. Vivo N+A 20ms Clustered & Distributed
# batch_nonlinearity_analysis('vivo_N+A_20ms', '_clus_invivo_singclus_t20ms', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs=10)

# batch_nonlinearity_analysis('vivo_N+A_20ms_distr', '_distr_invivo_singclus_t20ms', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs=10)

# # 8. Vivo N+A 40ms Clustered & Distributed
# batch_nonlinearity_analysis('vivo_N+A_40ms', '_clus_invivo_singclus_t40ms', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs=10)

# batch_nonlinearity_analysis('vivo_N+A_40ms_distr', '_distr_invivo_singclus_t40ms', ['basal', 'apical'],
#                             {'basal': ['dend', 'soma'], 'apical': ['dend', 'nexus']}, num_epochs=10)

# # saved_file = save_epsps_global()